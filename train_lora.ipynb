{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5da1a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.2.1+cu121 with CUDA 1201 (you have 2.2.1+cu118)\n",
      "    Python  3.10.11 (you have 3.10.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model_state_dict\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL, DDPMScheduler\n",
    "from diffusers.training_utils import compute_snr, cast_training_params\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b95cda97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5bc554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_csv(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    examples = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        # Use only the image filename, ignore bad prefix if needed\n",
    "        image_filename = os.path.basename(row[\"image_path\"])\n",
    "        image_path = os.path.join(\"dataset\", image_filename)\n",
    "\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"‚ùå Image not found: {image_path}\")\n",
    "            continue\n",
    "\n",
    "        parts = [\n",
    "            str(row.get(\"description\", \"\")).strip(),\n",
    "            \"with side salad\" if str(row.get(\"Beilagensalat\", \"\")).lower() in [\"yes\", \"ja\"] else \"\",\n",
    "            \"with regional apple\" if str(row.get(\"Regio Apfel\", \"\")).lower() in [\"yes\", \"ja\"] else \"\",\n",
    "            f\"type: {row.get('type', '')}\",\n",
    "            f\"diet: {row.get('diet', '')}\",\n",
    "            f\"served at {row.get('mensa', '')}\"\n",
    "        ]\n",
    "\n",
    "        prompt = \", \".join([p for p in parts if p])\n",
    "        examples.append({\"image\": Image.open(image_path).convert(\"RGB\"), \"text\": prompt})\n",
    "\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e475ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image': <PIL.Image.Image image mode=RGB size=3840x2880 at 0x2C9ABD0F610>, 'text': 'Apfelstrudel Vanillesauce Sauerkirschkompott, type: Essen 1, diet: Vegetarisch, served at Mensa Institutsviertel'}, {'image': <PIL.Image.Image image mode=RGB size=3468x4624 at 0x2C9ABD0F700>, 'text': 'Aglio Spaghetti mit getrockneten Tomaten, type: Schneller Teller, diet: Vegan, served at Mensa Institutsviertel'}]\n"
     ]
    }
   ],
   "source": [
    "test_examples = load_from_csv(\"./dataset/dataset.csv\")\n",
    "print(test_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5084910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_transform(example, tokenizer, resolution):\n",
    "    encoding = tokenizer(\n",
    "        example[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Squeeze to get (seq_len,) shape instead of (1, seq_len)\n",
    "    example[\"input_ids\"] = encoding[\"input_ids\"].squeeze(0)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(resolution, interpolation=Image.BILINEAR),\n",
    "        transforms.CenterCrop(resolution),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.5]),\n",
    "    ])\n",
    "    example[\"pixel_values\"] = transform(example[\"image\"])\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "302342bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "resolution = 512\n",
    "\n",
    "# Set paths for the dataset and model output\n",
    "dataset_csv = \"./dataset/dataset.csv\"  # CSV with metadata and paths\n",
    "output_dir = \"./lora_cafeteria_output\"  # Where to save LoRA weights\n",
    "pretrained_model = \"CompVis/stable-diffusion-v1-4\"  # Pretrained SD base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5144e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\monster\\miniconda3\\envs\\automlenv\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(pretrained_model, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(pretrained_model, subfolder=\"text_encoder\").to(device)\n",
    "vae = AutoencoderKL.from_pretrained(pretrained_model, subfolder=\"vae\").to(device)\n",
    "unet = UNet2DConditionModel.from_pretrained(pretrained_model, subfolder=\"unet\").to(device)\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5a317ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoencoderKL(\n",
       "  (encoder): Encoder(\n",
       "    (conv_in): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (down_blocks): ModuleList(\n",
       "      (0): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (downsamplers): ModuleList(\n",
       "          (0): Downsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): DownEncoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-1): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(512, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (conv_in): Conv2d(4, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (up_blocks): ModuleList(\n",
       "      (0-1): 2 x UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0-2): 3 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (upsamplers): ModuleList(\n",
       "          (0): Upsample2D(\n",
       "            (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): UpDecoderBlock2D(\n",
       "        (resnets): ModuleList(\n",
       "          (0): ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 256, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "            (conv_shortcut): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (1-2): 2 x ResnetBlock2D(\n",
       "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (nonlinearity): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (mid_block): UNetMidBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0): Attention(\n",
       "          (group_norm): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (to_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (to_out): ModuleList(\n",
       "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (1): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm2): GroupNorm(32, 512, eps=1e-06, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (conv_norm_out): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
       "    (conv_act): SiLU()\n",
       "    (conv_out): Conv2d(128, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (quant_conv): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (post_quant_conv): Conv2d(4, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Freeze all layers except LoRA\n",
    "unet.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "vae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be2e50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "unet.add_adapter(lora_config)\n",
    "cast_training_params(unet, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32a6d216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 2\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff31319f3c704067ba26ceaba66804ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([77])\n",
      "torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "examples = load_from_csv(dataset_csv)\n",
    "dataset = Dataset.from_list(examples)\n",
    "print(dataset)\n",
    "dataset = dataset.map(lambda e: tokenize_and_transform(e, tokenizer, resolution))\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"pixel_values\"])\n",
    "\n",
    "sample = dataset[0]\n",
    "print(type(sample[\"input_ids\"]))          # should be torch.Tensor\n",
    "print(sample[\"input_ids\"].shape)          # should be (77,)\n",
    "print(sample[\"pixel_values\"].shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d6218f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.stack([x[\"input_ids\"] for x in batch]),\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ff4b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, unet.parameters()), lr=1e-4)\n",
    "\n",
    "unet, optimizer, dataloader = accelerator.prepare(unet, optimizer, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4feda2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\monster\\miniconda3\\envs\\automlenv\\lib\\site-packages\\diffusers\\models\\attention_processor.py:1279: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  hidden_states = F.scaled_dot_product_attention(\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 complete. Loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:09<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 complete. Loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:08<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 complete. Loss: 0.1605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 complete. Loss: 0.0379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 complete. Loss: 0.1746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 complete. Loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 complete. Loss: 0.2042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 complete. Loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 complete. Loss: 0.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:07<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 complete. Loss: 0.2331\n"
     ]
    }
   ],
   "source": [
    "unet.train()\n",
    "for epoch in range(10):\n",
    "    for step, batch in enumerate(tqdm(dataloader)):\n",
    "        with accelerator.accumulate(unet):\n",
    "            # VAE encode\n",
    "            latents = vae.encode(batch[\"pixel_values\"].to(device)).latent_dist.sample()\n",
    "            latents = latents * vae.config.scaling_factor\n",
    "            noise = torch.randn_like(latents)\n",
    "            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (latents.shape[0],), device=device).long()\n",
    "            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "            encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(device))[0]\n",
    "            model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "            loss = F.mse_loss(model_pred, noise)\n",
    "\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    print(f\"Epoch {epoch + 1} complete. Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d3b3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA fine-tuning complete.\n"
     ]
    }
   ],
   "source": [
    "accelerator.wait_for_everyone()\n",
    "lora_state_dict = get_peft_model_state_dict(unet)\n",
    "torch.save(lora_state_dict, os.path.join(output_dir, \"pytorch_lora_weights.bin\"))\n",
    "print(\"LoRA fine-tuning complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
